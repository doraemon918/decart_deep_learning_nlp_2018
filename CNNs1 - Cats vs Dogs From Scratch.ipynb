{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "cfg.gpu_options.per_process_gpu_memory_fraction=0.333\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras\n",
    "\n",
    "#### @author Alec Chapman\n",
    "\n",
    "This tutorial was adapted from [this keras blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "\n",
    "The data comes from a [Kaggle competition to classify images as being cats or dogs. The data can be downloaded [here](https://www.kaggle.com/c/dogs-vs-cats/data) after signing into Kaggle (either via a Kaggle account or Google, Facebook, or Yahoo!).\n",
    "\n",
    "## What is Keras?\n",
    "\n",
    "Keras is a high-level deep learning API written in Python. Keras uses [TensorFlow](https://www.tensorflow.org/) (Google), [CNTK](https://github.com/Microsoft/cntk) (Microsoft), or [Theano](http://deeplearning.net/software/theano/) (University of Montreal) as a backend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some modules needed for our tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import RMSprop, Adagrad, Adam\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that I'm using the TensorFlow Backend\n",
    "\n",
    "This is controlled by an environment variable and is set in\n",
    "```bash\n",
    "/home/user/.keras/keras.json\n",
    "```\n",
    "which for me has the following content:\n",
    "\n",
    "```json\n",
    "\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"floatx\": \"float32\",\n",
    "    \"image_data_format\": \"channels_last\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATADIR = '/home/jovyan/DATA/keras_cat_dog/data'\n",
    "import getpass\n",
    "if getpass.getuser() == 'alec':\n",
    "    DATADIR = \"./data_alec/cats_vs_dogs/\"\n",
    "    MODELDIR = './saved_models'\n",
    "else:\n",
    "    DATADIR = os.path.join(os.path.expanduser('~'), 'DATA/DeepLearning/data/cats_vs_dogs/')\n",
    "    MODELDIR = os.path.join(os.path.expanduser('~'), 'DATA/DeepLearning/saved_models')\n",
    "TRAINDIR = os.path.join(DATADIR, 'train')\n",
    "VALDIR = os.path.join(DATADIR, 'val')\n",
    "assert os.path.exists(DATADIR)\n",
    "assert os.path.exists(MODELDIR)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "## CNNs for Computer Vision\n",
    "In this tutorial we'll build a Convolutional Neural Network to solve the age-old problem: Is it a **dog**, or a **cat**? \n",
    "\n",
    "Here's what we'll do today:\n",
    "- First, we'll look at what it actually means to deal with images in machine learning. \n",
    "- Then we'll starting using Keras, a great library that providers a higher-level API to sit on top of TensorFlow. \n",
    "- Finally, we'll train our model (for a bit) and then use a pre-trained model to classify a batch of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Images in Python\n",
    "\n",
    "Convolutional neural networks (CNNs) are often associated with computer vision. They're great at detecting edges, shapes, and higher-level features in images and using those findings to make a decision, such as classifying between cats and dogs. But how do we actually get these images into the neural net?\n",
    "\n",
    "Basically, images can be seen as arrays of pixels. If we flatten them, it would be one long array of numbers, where each array corresponds to a pixel. CNNs allow us to keep a grid-like shape rather than dealing with flat, 1-dimensional arrays. Specifically, our images will look like this:\n",
    "\n",
    "   ##### Height x Width x Channels \n",
    "where channels corresponds to the color channels (3 for RGB, 1 for grayscale). So a list of images being fed into a neural network will look like this:\n",
    "   ##### # of images x Height x Width x Channels\n",
    "   \n",
    "This array of matrices is often called a *tensor*.\n",
    "   \n",
    "Let's look at some examples.\n",
    "\n",
    "A great library for working with images in Python is the [Python Image Library](https://pillow.readthedocs.io/en/4.2.x/), or **PIL** (actually now **Pillow**, a fork of PIL).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "example = os.path.join(TRAINDIR, 'cat', 'cat.12497.jpg')\n",
    "img = Image.open(example)\n",
    "print(\"Width, height\")\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great for a human. Now let's convert it to something the computer can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(img)\n",
    "print(\"Height, width, Channels\")\n",
    "print(arr.shape)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And back...\n",
    "example = Image.fromarray(arr)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIL** offers great utility for working with images. Now let's look at Keras.\n",
    "\n",
    "### Keras \n",
    "[Keras](https://keras.io/) is an API that allows you to work with [TensorFlow](https://www.tensorflow.org/) or [Theano](http://deeplearning.net/software/theano/) in a much more user-friendly way. Per their description:\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "- Runs seamlessly on CPU and GPU.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "We'll get to neural nets in a minute. First let's look at some of the image functions Keras offers.\n",
    "\n",
    "##### Image Generators\n",
    "It's not always the case you have a bunch of data to train with. One solution to this is called **data augmentation**, where you create alterations of your existing data to provide more examples for your classifier. With images, that means that we'll stretch, augment, and crop the images so that we have a bunch of different versions of each of our images.\n",
    "\n",
    "Keras also offers a great utility called `flow_from_directory` that will allow us to put images in folders divided by class and Keras will automatically load them, know their label, and convert them into arrays to train/test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAINDIR,  # this is the target directory\n",
    "        target_size=(227, 227),  # all images will be resized to 227 x 227\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# No distorting for testing!\n",
    "# Instead, we'll rescale so that pixel values are between 0 and 1\n",
    "# Which, trust me, is very necessary.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) \n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        VALDIR,\n",
    "        target_size=(227, 227),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(VALDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates 16 images from the images found in TRAINDIR. Let's look at a few of them\n",
    "print(x_batch.shape)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nimg = x_batch.shape[0]\n",
    "\n",
    "fig = plt.figure(figsize=(18,9))\n",
    "for i in range(len(x_batch)):\n",
    "    x = x_batch[i]\n",
    "    ax = plt.subplot2grid((nimg//ncols+1, ncols), (i//ncols,i%ncols))\n",
    "    ax.imshow(x)\n",
    "    #img = array_to_img(x)\n",
    "    #img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of these get distorted. While this may look weird to us, it forces the classifier to look for other features that will allow it to recognize cats vs. dogs even with these strange distortions.\n",
    "\n",
    "Now, let's finally get to our trainer!\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "CNNs are special because of what's called a Convolutional Layer. See our presentation for the details.\n",
    "\n",
    "Here is the architecture that we're going to use, based on [Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). It is probably too deep for the problem, and we'll have to look out for overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "The [Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) model was one of the first famous \"deep\" architectures for image processing. It won the 2012 ImageNet competition and while CNNs have grown deeper and more complex, this is a good example of a multi-layered CNN. Here's a diagram of the complete architecture:\n",
    "\n",
    "The model above is based on AlexNet. Let's walk through each part of our model. See the [Keras documentation](keras.io) for more details on the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sequential` model\n",
    "The `Sequential` model is a linear stack of layers. We'll build our model by subsequently adding on additional layers to this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "A common design pattern in CNNs is a combination of these four layers:\n",
    "- 2D Convolutional\n",
    "- Activation function (ReLU)\n",
    "- Max Pooling\n",
    "\n",
    "\n",
    "#### **2D Convolutional Layer**\n",
    "![Example of Convolution](./images/convolution.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer 1\")\n",
    "model.add(Conv2D(\n",
    "                filters=96,\n",
    "                kernel_size=11,\n",
    "                strides=4,\n",
    "                padding='valid',\n",
    "                input_shape= \\\n",
    "                    (227, 227, 3),\n",
    "                data_format='channels_last')\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Non-Linear Activation Function**\n",
    "![ReLU](./images/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Max Pooling**\n",
    "![Max Pooling](./images/max_pooling.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(\n",
    "                      pool_size=(3, 3),\n",
    "                      strides=(2,2),\n",
    "                      data_format='channels_last')\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we have some combination of these layers, we'll call it a **Convolutional Layer**. So after our first Convolutional Layer, here's what our model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add four more similar layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer 2\")\n",
    "model.add(Conv2D(256, 5, strides=1, padding='valid', data_format='channels_last'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "          pool_size=(3, 3),\n",
    "          strides=(2,2),\n",
    "           data_format='channels_last')\n",
    "         )\n",
    "\n",
    "print(\"Layer 3\")\n",
    "model.add(Conv2D(\n",
    "           384, 3,\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           data_format='channels_last')\n",
    "         )\n",
    "\n",
    "print(\"Layer 4\")\n",
    "model.add(Conv2D(\n",
    "                 256, 3,\n",
    "                 strides=1,\n",
    "                 padding='valid',\n",
    "                 data_format='channels_last')\n",
    "          )\n",
    "\n",
    "\n",
    "print(\"Layer 5\")\n",
    "model.add(Conv2D(256, 3, strides=1, padding='valid', data_format='channels_last'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected and Dropout Layers\n",
    "At this point, we *flatten* our inputs so that we're now dealing with 1-dimensional vectors.\n",
    "\n",
    "#### Fully-Connected (Dense)\n",
    "![Fully-Connected Layer](./images/fully_connected.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout Layers\n",
    "![Dropout Layer](./images/dropout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "We then finally create an output layer. This layer will have **N** elements, where **N** is the number of classes we are trying to predict. In this case, our task is binary, so N=1.\n",
    "![Output Layer](./images/output.png)\n",
    "\n",
    "And we use a **sigmoid function** to squash our output value to be between 0 and 1, which we interpret as probability.\n",
    "![Output Layer](./images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our complete model looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "print(\"Layer 1\")\n",
    "model.add(Conv2D(\n",
    "                filters=96,\n",
    "                kernel_size=11,\n",
    "                strides=4,\n",
    "                padding='valid',\n",
    "                input_shape=(227, 227, 3)\n",
    "                )\n",
    "          )\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "                      pool_size=(3, 3),\n",
    "                      strides=(2,2))\n",
    "         )\n",
    "\n",
    "print(\"Layer 2\")\n",
    "model.add(Conv2D(256, 5, strides=1, padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "          pool_size=(3, 3),\n",
    "          strides=(2,2))\n",
    "         )\n",
    "\n",
    "print(\"Layer 3\")\n",
    "model.add(Conv2D(\n",
    "           384, 3,\n",
    "           strides=1,\n",
    "           padding='valid')\n",
    "         )\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "print(\"Layer 4\")\n",
    "model.add(Conv2D(\n",
    "                 256, 3,\n",
    "                 strides=1,\n",
    "                 padding='valid')\n",
    "          )\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "print(\"Layer 5\")\n",
    "model.add(Conv2D(256, 3, strides=1, padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1000))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer=RMSprop(lr=0.001),\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step before training is to **compile** the model, which defines configures the learning process. This defines:\n",
    "- The optimization algorithm\n",
    "- A loss function\n",
    "- A list of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='rmsprop',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now, we're finally ready to train! We provide our compiled model with our train and validation generators, which will read in the images we have in our data directory and perform image transformation, and will train for a total of 5 epochs. We'll then look at the training and validation scores.\n",
    "\n",
    "However, training a deep network with images takes a **long** time. So instead, we trained a model for you that you'll be able to use post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000 // batch_size, # Number of batches\n",
    "    epochs=1, \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=800//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('saved_models/cats_vs_dogs_not_trained.h5')\n",
    "\n",
    "#import pickle\n",
    "#with open('logs/history_cats_vs_dogs.pkl', 'wb') as f:\n",
    "#    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you go - Design Choices\n",
    "One of the main tasks in machine learning is testing out different design choices. This involes identifying various components of a model that can be changed/adjusted and testing which combinations work best for the task at hand.\n",
    "\n",
    "With CNNs, here are some model components to consider:\n",
    "\n",
    "- **Architecture**\n",
    "   - Number of layers\n",
    "   - Types of layers\n",
    "   - Order of layers\n",
    "   - Which activation functions to use\n",
    "- **Convolutional Hyperparameters**\n",
    "    - Number of filters\n",
    "    - Kernel size (filter size)\n",
    "    - Stride size\n",
    "    - Padding\n",
    "- **MaxPooling Hyperparameters**\n",
    "    - Pool size\n",
    "    - Strides size\n",
    "    - Padding\n",
    "- **Fully-Connected and Dropout Hyperparameters**\n",
    "    - Number of neurons\n",
    "    - Dropout value\n",
    "- **Training**\n",
    "    - Number of epochs\n",
    "    - Learning rate\n",
    "    - Optimization algorithm\n",
    "    - Batch size\n",
    "    \n",
    "#### Build your own CNN\n",
    "Try building a few different models by testing out different parameters as above. Start small and just try out a few different values - look at the docstrings below or the [Keras documentation](keras.io) to see the argument values that represent the hyperparameters. It can be a little tricky just to get all of the different layers to fit together, so that's a good first step. Then, if you have the time/RAM, try actually training a few different models and see if it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Conv2D)\n",
    "# help(MaxPooling2D)\n",
    "# help(Activation)\n",
    "# help(Dense)\n",
    "# help(Dropout)\n",
    "# help(model.compile)\n",
    "# help(keras.optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Sequential()\n",
    "\n",
    "### Add layers here ###\n",
    "# my_model.add(...)\n",
    "#######################\n",
    "\n",
    "### Compile your model ###\n",
    "# my_model.compile(loss='binary crossentropy',\n",
    "#     optimizer=...\n",
    "#     metrics=['accuracy'])\n",
    "#######################\n",
    "\n",
    "# print(my_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train your model ###\n",
    "# num_epochs = ...\n",
    "# batch_size = ...\n",
    "# my_history = my_model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=2000 // batch_size, # Number of batches\n",
    "#     epochs=num_epochs, \n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=800//batch_size)\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate your model ###\n",
    "# loss, acc = my_model.evaluate_generator(validation_generator, steps=800//batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up\n",
    "Deep neural nets like this can take a *long* time to train. With large datasets, models are sometimes trained for hours or days to get the best results.\n",
    "\n",
    "Next, we'll skip ahead to after training and see how we can use a pretrained model for predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
