{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Pneumonia Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Helper Packages\n",
    "\n",
    "#### We also need to load some nltk resources. These resources will help us clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jferraro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text contains a lot of noisy words that are simply there to help with fluency.\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic NLP Processing\n",
    "\n",
    "#### We want to coustomize tokenization so we have more control over our text and can remove unique textual forms of noise like dates, ages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def tokenize_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve our Corpus\n",
    "\n",
    "#### Let's pull in our corpus that we had serialized out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/differential-corpus.pkl','rb')\n",
    "corpus = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Transformation\n",
    "\n",
    "#### Let's build a dataframe so we can select case types easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PNA1</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2996-12-2**] 10:25 AM\\n   ...</td>\n",
       "      <td>PNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PNA2</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2850-2-14**] 10:22 PM\\n   ...</td>\n",
       "      <td>PNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PNA3</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2631-10-3**] 9:52 AM\\n    ...</td>\n",
       "      <td>PNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PNA4</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2584-11-21**] 11:17 AM\\n  ...</td>\n",
       "      <td>PNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PNA5</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2584-11-21**] 11:17 AM\\n  ...</td>\n",
       "      <td>PNA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case                                           document label\n",
       "0  PNA1  \\n\\n\\n     DATE: [**2996-12-2**] 10:25 AM\\n   ...   PNA\n",
       "1  PNA2  \\n\\n\\n     DATE: [**2850-2-14**] 10:22 PM\\n   ...   PNA\n",
       "2  PNA3  \\n\\n\\n     DATE: [**2631-10-3**] 9:52 AM\\n    ...   PNA\n",
       "3  PNA4  \\n\\n\\n     DATE: [**2584-11-21**] 11:17 AM\\n  ...   PNA\n",
       "4  PNA5  \\n\\n\\n     DATE: [**2584-11-21**] 11:17 AM\\n  ...   PNA"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=('case', 'document', 'label'))\n",
    "for case, document in corpus.items():\n",
    "    if 'PNA' in case:\n",
    "        df = df.append({'case': case, 'document': document, 'label': 'PNA'}, ignore_index=True)\n",
    "    elif 'COPD' in case:\n",
    "        df = df.append({'case': case, 'document': document, 'label': 'COPD'}, ignore_index=True)\n",
    "    else: # CHF\n",
    "        df = df.append({'case': case, 'document': document, 'label': 'CHF'}, ignore_index=True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vocabulary\n",
    "\n",
    "#### Using a data frame makes it easy to select between different cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to count our vocabulary to determine the frequency of words we want to keep\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('right', 1479), ('left', 1311), ('chest', 1304), ('clip', 1235), ('reason', 1143), ('contrast', 1102), ('ct', 1036), ('lobe', 947), ('pleural', 858), ('pneumonia', 842), ('lower', 702), ('lung', 655), ('number', 646), ('report', 646), ('date', 641), ('final', 638), ('radiology', 633), ('effusion', 616), ('examination', 614), ('bilateral', 555), ('underlying', 542), ('upper', 538), ('old', 534), ('year', 505), ('small', 502), ('within', 500), ('condition', 499), ('medical', 496), ('pulmonary', 493), ('ap', 487), ('impression', 471), ('tube', 469), ('portable', 468), ('effusions', 417), ('seen', 413), ('interval', 399), ('prior', 391), ('pm', 375), ('opacities', 358), ('indication', 351), ('diagnosis', 350), ('comparison', 346), ('unchanged', 339), ('please', 335), ('consolidation', 332), ('admitting', 328), ('study', 319), ('abdomen', 317), ('evidence', 305), ('evaluate', 297), ('eval', 296), ('iv', 286), ('man', 285), ('opacity', 277), ('new', 275), ('change', 274), ('optiray', 274), ('woman', 271), ('noted', 268), ('edema', 268), ('history', 257), ('heart', 248), ('findings', 246), ('consistent', 246), ('mass', 241), ('fluid', 240), ('wcontrast', 238), ('atelectasis', 237), ('pelvis', 234), ('present', 231), ('patient', 228), ('likely', 227), ('worsening', 222), ('sp', 221), ('view', 216), ('unremarkable', 214), ('also', 211), ('chf', 209), ('increased', 208), ('multiple', 201), ('large', 200), ('assess', 198), ('compared', 198), ('patchy', 196), ('pna', 195), ('amt', 194), ('pneumothorax', 194), ('infiltrate', 194), ('images', 193), ('placement', 193), ('without', 192), ('appearance', 191), ('mediastinal', 186), ('diffuse', 186), ('air', 186), ('technique', 184), ('normal', 184), ('cm', 181), ('size', 181), ('tip', 180)]\n"
     ]
    }
   ],
   "source": [
    "df_pna = df.loc[df['label'] == 'PNA']\n",
    "for index, row in df_pna.iterrows():\n",
    "        document = row['document']\n",
    "        tokens = tokenize_doc(document)\n",
    "        vocab.update(tokens)\n",
    "print(vocab.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : 3375\n",
      "After : 2619\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "print('Before : %d' % len(vocab))\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print('After : %d' % len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'data/pna_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
