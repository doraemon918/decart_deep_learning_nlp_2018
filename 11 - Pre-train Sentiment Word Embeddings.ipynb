{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Sentiment Word Enbeddings using Word2Vew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "    # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Movie Reviews Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'data/movie_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 58109\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "positive_lines = process_docs('data/txt_sentoken/pos', vocab, True)\n",
    "negative_lines = process_docs('data/txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_lines + positive_lines\n",
    "print('Total training sentences: %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25767\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'data/movie_embeddings_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
